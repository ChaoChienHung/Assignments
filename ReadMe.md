# AI Research Assistant â€“ Development Notes

æœ¬æ–‡ä»¶ç´€éŒ„ **AI Research Assistant** å°ˆæ¡ˆçš„è¨­è¨ˆæ€è·¯ã€ç¨‹å¼æ¶æ§‹ã€å¾…å®ŒæˆåŠŸèƒ½èˆ‡ Mock å¯¦ä½œç´°ç¯€ã€‚  
ç›®æ¨™ï¼šä»¥ **çˆ¬èŸ² â†’ çµæ§‹åŒ–æŠ½å– â†’ï¼ˆå¯é¸ï¼‰LLM Function Calling â†’ å›ºå®šé‚è¼¯åˆ†æ** çš„æ–¹å¼ï¼Œå®Œæˆå°å‹ç ”ç©¶ç³»çµ±ã€‚

---

## ğŸ—ï¸ æ•´é«”æ¶æ§‹èˆ‡æµç¨‹

```
flowchart TD
A[Raw Data (HTML/Plain Text)] --> B[Raw Text Cleaning]
B --> C[HTML Parser â†’ JSON Format]
C --> D[AIResearchAssistant Class]
D --> D1[Structured Extraction (Pydantic Schema)]
D --> D2[Store Extracted Articles]
D --> D3[Function Calling Layer]
D3 -->|compare_technologies() or trace_evolution()| E[External Tool / Gemini API]
D3 -->|Mock Mode| F[Basic JSON/Dict Extraction + Print Summary]
```
- **Raw Text Cleaning**ï¼šå° HTML/ç´”æ–‡å­—é€²è¡Œåˆæ­¥æ¸…ç†ï¼ˆç§»é™¤æ¨™ç±¤ã€å¤šé¤˜ç©ºæ ¼ç­‰ï¼‰ã€‚
- **HTML Parser â†’ JSON Format**ï¼šè§£æ HTMLï¼Œå°‡çµæ§‹åŒ–è³‡è¨Šï¼ˆæ¨™é¡Œã€Headerã€æ®µè½ç­‰ï¼‰è½‰æˆ JSON/dictã€‚
- **AIResearchAssistant** Classï¼šæ ¸å¿ƒç®¡ç†æ¨¡çµ„ï¼Œå…§å«ï¼š
    - **Structured Extraction** (Pydantic Schema)ï¼šåˆ©ç”¨ Pydantic Schema é©—è­‰èˆ‡å­˜æ”¾æ–‡ç« è³‡æ–™ã€‚
    - **Store Extracted Articles**ï¼šé›†ä¸­å„²å­˜æ‰€æœ‰å·²è™•ç†æ–‡ç« ï¼Œä¾¿æ–¼å¾ŒçºŒæ“ä½œã€‚
    - **Function Calling Layer**ï¼šæ ¹æ“š LLM è¼¸å…¥è‡ªå‹•æ±ºå®šï¼š
        - **å‘¼å«å¤–éƒ¨å·¥å…·**ï¼ˆå¦‚ Gemini APIï¼‰
        - **æˆ–é€²å…¥ Mock æ¨¡å¼**ï¼ˆä»¥ JSON/dict è¼¸å‡ºä¸¦åšç°¡å–®æ‘˜è¦ï¼‰ã€‚

---

## ğŸ“– Wikipedia WebScraper

é€™æ˜¯ä¸€å€‹åŸºæ–¼ **Python + Crawl4AI + OpenAI API** çš„ **éåŒæ­¥ Wikipedia çˆ¬èŸ²**ï¼Œå¯ä»¥æ‰¹é‡æŠ“å–æ–‡ç« å…§å®¹ï¼Œä¸¦å°‡å…§å®¹è½‰æ›æˆä¹¾æ·¨çš„æ–‡å­—ï¼Œä»¥åˆ©å¾ŒçºŒé€²è¡Œ NLP æˆ– LLM çŸ¥è­˜åº«å»ºæ§‹ã€‚

---

### ğŸš€ åŠŸèƒ½ç‰¹è‰²

- æ”¯æ´ **å¤šç¶²å€éåŒæ­¥çˆ¬å–**ï¼ˆ`asyncio`ï¼‰
- **é€Ÿç‡é™åˆ¶ï¼ˆRate Limitingï¼‰**ï¼šé¿å…éåº¦è«‹æ±‚
- å›å‚³çµæ§‹åŒ–è³‡è¨Šï¼ˆæ¨™é¡Œã€å­—æ•¸ã€é€£çµæ•¸é‡ã€è™•ç†æ™‚é–“ï¼‰
- å…§å»º **Markdown â†’ ç´”æ–‡å­—** æ¸…ç†åŠŸèƒ½
- æ”¯æ´ **å®‰å…¨ API Key è¼¸å…¥** èˆ‡ **OpenAI client é©—è­‰**
- å¯æ“´å……è‡³ **è‡ªå‹• chunking / LLM-based extraction**

---

### ğŸ“¦ å®‰è£éœ€æ±‚

#### è«‹å…ˆå®‰è£å¿…è¦å¥—ä»¶ï¼š

```bash
pip install crawl4ai ratelimit beautifulsoup4 markdown openai pandas requests
```

#### ğŸ›  ä½¿ç”¨æ–¹å¼
1. API Key è¨­å®š
è«‹åœ¨åŸ·è¡Œç¨‹å¼æ™‚è¼¸å…¥ API Keyï¼Œæˆ–æ˜¯é å…ˆåœ¨ç’°å¢ƒè®Šæ•¸ä¸­è¨­å®šï¼š
`export OPENAI_API_KEY="your_key_here"`

2. å»ºç«‹ Scraper ä¸¦åŸ·è¡Œ
```python
import asyncio
from scraper import WikipediaScraper  # å‡è¨­ä½ æŠŠ class å­˜åœ¨ scraper.py

urls = [
    "https://en.wikipedia.org/wiki/Natural_language_processing",
    "https://en.wikipedia.org/wiki/Machine_learning"
]

scraper = WikipediaScraper(base_urls=urls)

results = asyncio.run(scraper.scrape_multiple())

for r in results:
    print(r["title"], r["markdown_length"], "chars")
```

#### ğŸ“‚ å›å‚³çµæœæ ¼å¼

æ¯ç¯‡æ–‡ç« çš„çµæœæœƒæ˜¯ JSON-like dictï¼š
```bash
{
  "title": "Natural language processing",
  "markdown": "...",
  "html_length": 15234,
  "markdown_length": 8921,
  "links_found": 245,
  "crawl_time": 1735712456.2381
}
```

#### ğŸ§¹ æ¸…ç†å…§å®¹
ä½ å¯ä»¥ä½¿ç”¨ clean_content() å°‡ Markdown è½‰æ›æˆä¹¾æ·¨çš„ç´”æ–‡å­—ï¼š
```python
raw_markdown = results[0]["markdown"]
clean_text = scraper.clean_content(raw_markdown)

print(clean_text[:500])  # é¡¯ç¤ºå‰ 500 å­—
```
#### ğŸ”’ OpenAI Client å»ºç«‹èˆ‡æ¸¬è©¦

æ­¤å°ˆæ¡ˆå…§å»º `create_secure_openai_client()`ï¼Œæœƒï¼š
- å¾ç’°å¢ƒè®Šæ•¸è®€å– API key
- æ¸¬è©¦æ˜¯å¦èƒ½æ­£ç¢ºé€£ç·š
- å›å‚³ä¸€å€‹ OpenAI client

ç¯„ä¾‹ï¼š
```python
from scraper import create_secure_openai_client

client = create_secure_openai_client()

if client:
    response = client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[{"role": "user", "content": "Hello, world!"}]
    )
    print(response.choices[0].message)
```

---

## ğŸ“¦ Data Contractsï¼ˆPydantic Schemasï¼‰

### Article Schema
```python
from pydantic import BaseModel, Field
from typing import List

class WikipediaExtraction(BaseModel):
    title: str = Field(description="Article's Title")
    description: str = Field(description="Article's Summary or Description or Overview")
    advantages: List[str] = Field(description="The advantages of the topic mentioned in the article")
    disadvantages: List[str] = Field(description="Known challenges or limitations")
    related_concepts: List[str] = Field(description="Related technology, see also")
    notable_methods: List[str] = Field(description="Notable methods, models, or techniques in this area")
```
> è¨»ï¼šå¯¦éš›æ¬„ä½å¯ä¾ä½œæ¥­æœ€çµ‚ schemaï¼ˆå¦‚ `summary`, `key_concepts`, `applications`ï¼‰èª¿æ•´ã€‚

---

## ğŸ§  AIResearchAssistant Classï¼ˆç‹€æ…‹èˆ‡è¡Œç‚ºï¼‰

```python
from typing import List, Dict, Optional

class AIResearchAssistant:
    def __init__(self, client=None, model: str = "gpt-4o-mini"):
        # ç®¡ç†æ‰€æœ‰æŠ½å–å‡ºä¾†çš„æ–‡ç« 
        self.articles: List[WikipediaExtraction] = []
        self.client = client
        self.model = model

    # ---- Data In/Out ----
    def add_article(self, article: WikipediaExtraction) -> None:
        self.articles.append(article)

    def list_titles(self) -> List[str]:
        return [a.title for a in self.articles]

    def get_article_by_title(self, title: str) -> WikipediaExtraction:
        for a in self.articles:
            if a.title == title:
                return a
        raise ValueError(f"Article not found: {title}")

    def get_articles_by_category(self, category: str) -> List[WikipediaExtraction]:
        # category å¯èƒ½å°æ‡‰ related_conceptsï¼Œéœ€è‡ªå®šç¾©
        results = [a for a in self.articles if category in a.related_concepts]
        if not results:
            raise ValueError(f"No articles found for category: {category}")
        return results

    # ---- Extraction ----
    def extract_structured_data(self, content: str) -> WikipediaExtraction:
        """Use OpenAI structured output to extract data, fallback to mock if no API."""
        if not self.client:
            print("âš ï¸ Demo mode: Mock extraction.")
            return self.create_mock_wiki_extraction()

        schema = {
            "name": "wiki_extraction",
            "schema": WikipediaExtraction.model_json_schema(),
            "strict": False
        }

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system",
                     "content": "Please extract and structure the article strictly according to the schema."},
                    {"role": "user",
                     "content": f"Analyze the article: {content}"}
                ],
                response_format={"type": "json_schema", "json_schema": schema}
            )

            wiki_extraction = WikipediaExtraction.model_validate_json(
                response.choices[0].message.content
            )
            print(f"âœ… Extracted: {wiki_extraction.title}")
            return wiki_extraction

        except Exception as e:
            print(f"âŒ Extraction failed: {e}")
            return self.create_mock_wiki_extraction()

    def batch_extract(self, articles: List[Dict]) -> List[WikipediaExtraction]:
        extracted = []
        for article in articles:
            extracted.append(self.extract_structured_data(article['markdown']))
        return extracted

    def create_mock_wiki_extraction(self) -> WikipediaExtraction:
        return WikipediaExtraction(
            title="Null",
            description="Null",
            advantages=[],
            disadvantages=[],
            related_concepts=[],
            notable_methods=[]
        )
```

### Function Calling Layer (å°å¤–ä»‹é¢)
```python
def ask_ai(query: str, assistant: AIResearchAssistant):
    """
    Function Calling å±¤ï¼ŒAI Assistant èˆ‡å¤–éƒ¨å°è©±çš„å”¯ä¸€å…¥å£ã€‚
    - query: ä½¿ç”¨è€…çš„ä»»å‹™ (ex: "compare_technologies: LLM, RNN")
    - assistant: å·²ç¶“æ“æœ‰ structured articles çš„ AIResearchAssistant
    """
    if query.startswith("compare_technologies"):
        # Example: compare_technologies: LLM, RNN
        techs = query.split(":")[1].split(",")
        results = [assistant.get_article_by_title(t.strip()) for t in techs]
        return results

    elif query.startswith("trace_evolution"):
        # Example: trace_evolution: Deep Learning
        topic = query.split(":")[1].strip()
        return assistant.get_articles_by_category(topic)

    else:
        raise ValueError(f"Unknown query: {query}")
```

**Error Handling**ï¼š  
- è‹¥ `articles` ç‚ºç©ºæˆ–æ‰¾ä¸åˆ°æ¨™é¡Œ â†’ `ValueError`ã€‚  
- `category` å¯ç‚º `None`ï¼›ä½¿ç”¨æ™‚éœ€åšç©ºå€¼æª¢æŸ¥ã€‚  
- è¼¸å…¥ JSON â†’ ä»¥ `Article.model_validate()` é©—è­‰ã€‚  

---

## ğŸ”§ Function Calling Toolsï¼ˆå¤–éƒ¨å·¥å…·è¦æ ¼ï¼‰

> Function Calling ç²¾ç¥ï¼š**LLM æ±ºç­–**æ˜¯å¦å‘¼å«å·¥å…·ï¼Œä½†å·¥å…·æœ¬èº«æ˜¯**å›ºå®šã€å¯æ§**çš„é‚è¼¯ï¼›è‹¥ç„¡ API â†’ é€²å…¥ **Mock Mode**ã€‚  
> é€™å…©å€‹å‡½å¼ä½œç‚ºã€Œå¤–éƒ¨å·¥å…·ã€ï¼Œä¸å°è£åœ¨ Class å…§ï¼ˆç”± Class æä¾›è³‡æ–™çµ¦å®ƒå€‘ï¼‰ã€‚

### 1) `compare_technologies`
**Signature**
```python
def compare_technologies(article_a_json: str, article_b_json: str) -> str:
    """
    æ¯”è¼ƒå…©å€‹æŠ€è¡“æ–‡ç« ï¼ˆä»¥ JSON å­—ä¸²è¼¸å…¥ï¼‰ä¸¦å›å‚³ JSON å­—ä¸²çµæœã€‚

    Parameters:
        article_a_json (str): JSON string of Article
        article_b_json (str): JSON string of Article

    Returns:
        str: JSON string of ArticleComparison
    """
```
**è¼¸å…¥/è¼¸å‡ºæ ¼å¼**  
- **è¼¸å…¥**ï¼š`Article` ç‰©ä»¶åºåˆ—åŒ–å¾Œçš„ **JSON å­—ä¸²**ï¼ˆä¾¿æ–¼ LLM æˆ–è·¨æ¨¡çµ„å‚³éï¼‰ã€‚  
- **è¼¸å‡º**ï¼š`ArticleComparison` ç‰©ä»¶åºåˆ—åŒ–å¾Œçš„ **JSON å­—ä¸²**ã€‚  

**è¡Œç‚ºï¼ˆAPI / Mockï¼‰**  
- **API å¯ç”¨**ï¼šå¯å‘¼å« Gemini/OpenAI å°å…©ç¯‡æ–‡ç« åšå°æ¯”æ‘˜è¦èˆ‡æ¬„ä½å¡«å……ã€‚  
- **Mock**ï¼šæ“·å– `title`ã€`content` å‰ N å­—è£½ä½œæ‘˜è¦ï¼›`common_concepts/unique_*` å¯åŸºæ–¼ç°¡å–®é—œéµå­—é›†åˆæ¯”è¼ƒã€‚  

### 2) `trace_evolution`
**Signature**
```python
from typing import List
def trace_evolution(topic: str, articles_json: List[str]) -> str:
    """
    è¿½è¹¤å–®ä¸€ä¸»é¡Œåœ¨å¤šç¯‡æ–‡ç« ä¸­çš„æ¼”é€²ï¼ˆä»¥ JSON å­—ä¸²é™£åˆ—è¼¸å…¥ã€JSON å­—ä¸²è¼¸å‡ºï¼‰ã€‚

    Parameters:
        topic (str): æŠ€è¡“ä¸»é¡Œ
        articles_json (List[str]): list of JSON string of Article

    Returns:
        str: JSON string (e.g., {'timeline': [...], 'key_innovations': [...], 'notes': str})
    """
```
**è¡Œç‚ºï¼ˆAPI / Mockï¼‰**  
- **API å¯ç”¨**ï¼šè«‹ LLM ä¾æ™‚é–“/æ®µè½æ¨æ¸¬æ¼”é€²è„ˆçµ¡èˆ‡é‡Œç¨‹ç¢‘ã€‚  
- **Mock**ï¼šä»¥æ¨™é¡Œæ’åº + å–æ¯ç¯‡ç¬¬ä¸€æ®µä½œç‚ºã€Œé‡Œç¨‹ç¢‘æ‘˜è¦ã€ã€‚  

---

## ğŸ”„ Pipeline å»ºè­°ï¼ˆé¿å… globalï¼Œæ¸…æ¥šè³‡æ–™æµï¼‰

1. **çˆ¬èŸ²** â†’ å–å¾— raw HTML / textã€‚  
2. **æ¸…ç†** â†’ `clean_content`ï¼ˆç§»é™¤é›œè¨Šã€ä¿ç•™æ®µè½ï¼‰ã€‚  
3. **HTML Parser** â†’ æ“·å–æ¨™é¡Œ/æ®µè½ â†’ dictã€‚  
4. **Pydantic** â†’ é©—è­‰ â†’ `Article` ç‰©ä»¶ â†’ å­˜å…¥ `AIResearchAssistant.articles`ã€‚  
5. **Function Calling**ï¼š
   - å¾ Class å–å‡º `Article`ï¼Œè½‰æˆ **JSON å­—ä¸²**ï¼Œä½œç‚ºå·¥å…·çš„ **input arguments**ã€‚  
   - å‘¼å« `compare_technologies` / `trace_evolution`ï¼ˆAPI æˆ– Mockï¼‰ã€‚  
6. **çµæœå‘ˆç¾**ï¼šå°å‡º / å­˜æª” / å¯è¦–åŒ–ã€‚  

---

## âš ï¸ Error Handling ç­–ç•¥

- **æ²’æœ‰ä»»ä½• Article**ï¼šåœ¨å‘¼å«æ¯”è¼ƒ/æ¼”é€²å‰å…ˆæª¢æŸ¥ï¼Œå¦å‰‡ `ValueError("No articles available")`ã€‚  
- **æ‰¾ä¸åˆ°æŒ‡å®šæ¨™é¡Œ**ï¼š`get_article_by_title` æ‹‹å‡º `ValueError`ã€‚  
- **Category ç‚ºç©º**ï¼šå…è¨± `None`ï¼›åœ¨æª¢ç´¢ç‰¹å®šåˆ†é¡æ™‚é¡¯ç¤ºè­¦å‘Šæˆ–è·³éã€‚  
- **JSON åºåˆ—åŒ–/ååºåˆ—åŒ–**ï¼šä½¿ç”¨ `Article.model_validate_json()` èˆ‡ `model_dump_json()`ã€‚  

---

## âœ… å¾…å®Œæˆæ¸…å–®ï¼ˆChecklistï¼‰

- [ ] HTML Parserï¼š`mock_extract_from_html` çœŸå¯¦å¯¦ä½œï¼ˆBeautifulSoupï¼‰ã€‚  
- [ ] æ¸…ç†å‡½å¼ï¼š`clean_content`ï¼ˆç§»é™¤éæ­£æ–‡ã€éå¤šç©ºç™½ã€code fences ç­‰ï¼‰ã€‚  
- [ ] `AIResearchAssistant`ï¼šéŒ¯èª¤è™•ç†èˆ‡è¼”åŠ©æ–¹æ³•å®Œå–„ï¼ˆå¦‚ `remove_article`ã€`update_article`ï¼‰ã€‚  
- [ ] `compare_technologies`ï¼šAPI èˆ‡ Mock ç‰ˆæœ¬ï¼ˆè¼¸å…¥/è¼¸å‡º JSON å­—ä¸²ï¼‰ã€‚  
- [ ] `trace_evolution`ï¼šAPI èˆ‡ Mock ç‰ˆæœ¬ï¼ˆè¼¸å…¥/è¼¸å‡º JSON å­—ä¸²ï¼‰ã€‚  
- [ ] æ¸¬è©¦ï¼šç©ºè³‡æ–™ã€æ‰¾ä¸åˆ°æ¨™é¡Œã€category ç‚ºç©ºã€JSON æ ¼å¼éŒ¯èª¤ç­‰ã€‚  
- [ ] æ–‡ä»¶åŒ–ï¼šåœ¨ README ä¸­åŠ å…¥ä½¿ç”¨ç¤ºä¾‹èˆ‡ CLI/Notebook ç¯„ä¾‹ã€‚  

---

## ğŸ”— JSON æ ¼å¼ç¯„ä¾‹ï¼ˆArticleï¼‰

```json
{
  "title": "Radiosity (Computer Graphics)",
  "content": "Radiosity is a global illumination algorithm ...",
  "category": "Rendering"
}
```

---

## ğŸ“ ä½¿ç”¨å»ºè­°
- å…§éƒ¨é‹ç®—å»ºè­°ä»¥ **dict/Pydantic** é€²è¡Œï¼›èˆ‡ LLM æˆ–å·¥å…·å±¤äº¤äº’æ™‚å†è½‰ **JSON string**ã€‚  
- ä¿æŒå·¥å…·ï¼ˆå‡½å¼ï¼‰è¼¸å…¥/è¼¸å‡ºåš´æ ¼å®šç¾©ï¼Œæ–¹ä¾¿æ¸¬è©¦èˆ‡æ›¿æ›ï¼ˆAPI â†” Mockï¼‰ã€‚  
- é¿å…ä½¿ç”¨ global stateï¼›ç”± Class çµ±ä¸€ç®¡ç†ç‹€æ…‹èˆ‡è³‡æ–™ã€‚

ğŸ”® æœªä¾†æ“´å……å»ºè­°
- è³‡æ–™å„²å­˜ï¼šå°‡çµæœå­˜å…¥ SQLite / MongoDB / CSV
- è‡ªå‹• Chunkingï¼šä½¿ç”¨ RegexChunking / SlidingWindowChunking
- è³‡è¨Šæ“·å–ï¼šæ­é… LLMExtractionStrategy è‡ªå‹•ç”Ÿæˆçµæ§‹åŒ–çŸ¥è­˜
- å‰ç«¯å±•ç¤ºï¼šæ•´åˆæˆä¸€å€‹å°å‹ Web NotebookLM Demo
