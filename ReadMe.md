# AI Research Assistant ‚Äì Development Notes

Êú¨Êñá‰ª∂Á¥ÄÈåÑ **AI Research Assistant** Â∞àÊ°àÁöÑË®≠Ë®àÊÄùË∑Ø„ÄÅÁ®ãÂºèÊû∂Êßã„ÄÅÂæÖÂÆåÊàêÂäüËÉΩËàá Mock ÂØ¶‰ΩúÁ¥∞ÁØÄ„ÄÇ  
ÁõÆÊ®ôÔºö‰ª• **Áà¨Ëü≤ ‚Üí ÁµêÊßãÂåñÊäΩÂèñ ‚ÜíÔºàÂèØÈÅ∏ÔºâLLM Function Calling ‚Üí Âõ∫ÂÆöÈÇèËºØÂàÜÊûê** ÁöÑÊñπÂºèÔºåÂÆåÊàêÂ∞èÂûãÁ†îÁ©∂Á≥ªÁµ±„ÄÇ

---

## üèóÔ∏è Êï¥È´îÊû∂ÊßãËàáÊµÅÁ®ã

```
flowchart TD
A[URL] --> Wikipedia Parser --> B[Raw Text]
B[Raw Text] --> Wikipedia Parser clean_content (HTML Parser) --> C[JSON Format]
C --> D[Agent Class]
D --> D1[Structured Extraction (Pydantic Schema)]
D --> D2[Store Extracted Articles]
D --> D3[Function Calling Layer]
D3 -->|compare_technologies() or trace_evolution()| E[External Tool / Gemini API]
D3 -->|Mock Mode| F[Basic JSON/Dict Extraction + Print Summary]
```
- **Raw Text Cleaning**ÔºöÂ∞ç HTML/Á¥îÊñáÂ≠óÈÄ≤Ë°åÂàùÊ≠•Ê∏ÖÁêÜÔºàÁßªÈô§Ê®ôÁ±§„ÄÅÂ§öÈ§òÁ©∫Ê†ºÁ≠âÔºâ„ÄÇ
- **HTML Parser ‚Üí JSON Format**ÔºöËß£Êûê HTMLÔºåÂ∞áÁµêÊßãÂåñË≥áË®äÔºàÊ®ôÈ°å„ÄÅHeader„ÄÅÊÆµËêΩÁ≠âÔºâËΩâÊàê JSON/dict„ÄÇ
- **Agent** ClassÔºöÊ†∏ÂøÉÁÆ°ÁêÜÊ®°ÁµÑÔºåÂÖßÂê´Ôºö
    - **Structured Extraction** (Pydantic Schema)ÔºöÂà©Áî® Pydantic Schema È©óË≠âËàáÂ≠òÊîæÊñáÁ´†Ë≥áÊñô„ÄÇ
    - **Store Extracted Articles**ÔºöÈõÜ‰∏≠ÂÑ≤Â≠òÊâÄÊúâÂ∑≤ËôïÁêÜÊñáÁ´†Ôºå‰æøÊñºÂæåÁ∫åÊìç‰Ωú„ÄÇ
    - **Function Calling Layer**ÔºöÊ†πÊìö LLM Ëº∏ÂÖ•Ëá™ÂãïÊ±∫ÂÆöÔºö
        - **ÂëºÂè´Â§ñÈÉ®Â∑•ÂÖ∑**ÔºàÂ¶Ç Gemini APIÔºâ
        - **ÊàñÈÄ≤ÂÖ• Mock Ê®°Âºè**Ôºà‰ª• JSON/dict Ëº∏Âá∫‰∏¶ÂÅöÁ∞°ÂñÆÊëòË¶ÅÔºâ„ÄÇ

---

## üìñ Wikipedia WebScraper

ÈÄôÊòØ‰∏ÄÂÄãÂü∫Êñº **Python + Crawl4AI + OpenAI API** ÁöÑ **ÈùûÂêåÊ≠• Wikipedia Áà¨Ëü≤**ÔºåÂèØ‰ª•ÊâπÈáèÊäìÂèñÊñáÁ´†ÂÖßÂÆπÔºå‰∏¶Â∞áÂÖßÂÆπËΩâÊèõÊàê‰πæÊ∑®ÁöÑÊñáÂ≠óÔºå‰ª•Âà©ÂæåÁ∫åÈÄ≤Ë°å NLP Êàñ LLM Áü•Ë≠òÂ∫´Âª∫Êßã„ÄÇ

---

### üöÄ ÂäüËÉΩÁâπËâ≤

- ÊîØÊè¥ **Â§öÁ∂≤ÂùÄÈùûÂêåÊ≠•Áà¨Âèñ**Ôºà`asyncio`Ôºâ
- ÂõûÂÇ≥ÁµêÊßãÂåñË≥áË®äÔºàÊ®ôÈ°å„ÄÅÂ≠óÊï∏„ÄÅÈÄ£ÁµêÊï∏Èáè„ÄÅËôïÁêÜÊôÇÈñìÔºâ
- ÂÖßÂª∫ **Markdown ‚Üí Á¥îÊñáÂ≠ó** Ê∏ÖÁêÜÂäüËÉΩ
- ÊîØÊè¥ **ÂÆâÂÖ® API Key Ëº∏ÂÖ•** Ëàá **OpenAI client È©óË≠â**
- ÂèØÊì¥ÂÖÖËá≥ **Ëá™Âãï chunking / LLM-based extraction**
- **ÈÄüÁéáÈôêÂà∂ÔºàRate LimitingÔºâ**ÔºöÈÅøÂÖçÈÅéÂ∫¶Ë´ãÊ±Ç

---

## ‚úÖ ÂæÖÂÆåÊàêÊ∏ÖÂñÆÔºàChecklistÔºâ

- [ ] WikipediaExtraction Error Case Handling
- [ ] HTML ParserÔºö`mock_extract_from_html` ÁúüÂØ¶ÂØ¶‰ΩúÔºàBeautifulSoupÔºâ„ÄÇ
- [ ] Asynchronous Multiple Scraping
- [ ] Ê∏ÖÁêÜÂáΩÂºèÔºö`clean_content`ÔºàÁßªÈô§ÈùûÊ≠£Êñá„ÄÅÈÅéÂ§öÁ©∫ÁôΩ„ÄÅcode fences Á≠âÔºâ„ÄÇ  
- [ ] `Agent`ÔºöÈåØË™§ËôïÁêÜËàáËºîÂä©ÊñπÊ≥ïÂÆåÂñÑÔºàÂ¶Ç `remove_article`„ÄÅ`update_article`Ôºâ„ÄÇ  
- [ ] `compare_technologies`ÔºöAPI Ëàá Mock ÁâàÊú¨ÔºàËº∏ÂÖ•/Ëº∏Âá∫ JSON Â≠ó‰∏≤Ôºâ„ÄÇ  
- [ ] `trace_evolution`ÔºöAPI Ëàá Mock ÁâàÊú¨ÔºàËº∏ÂÖ•/Ëº∏Âá∫ JSON Â≠ó‰∏≤Ôºâ„ÄÇ  
- [ ] Ê∏¨Ë©¶ÔºöÁ©∫Ë≥áÊñô„ÄÅÊâæ‰∏çÂà∞Ê®ôÈ°å„ÄÅcategory ÁÇ∫Á©∫„ÄÅJSON Ê†ºÂºèÈåØË™§Á≠â„ÄÇ  
- [ ] Êñá‰ª∂ÂåñÔºöÂú® README ‰∏≠Âä†ÂÖ•‰ΩøÁî®Á§∫‰æãËàá CLI/Notebook ÁØÑ‰æã„ÄÇ
- [ ] `create_mock_wiki_extraction`
- [ ] `batch_extract`

### üì¶ ÂÆâË£ùÈúÄÊ±Ç

#### Dependencies
* crawl4ai>=0.2.0
* openai>=1.0.0
* pydantic>=2.0.0
* python-dotenv>=1.0.0
* requests>=2.25.0
* beautifulsoup4>=4.9.0

**Ë´ãÂÖàÂÆâË£ùÂøÖË¶ÅÂ•ó‰ª∂Ôºö**
  
```bash
pip install crawl4ai ratelimit beautifulsoup4 markdown openai pandas requests
```

#### üõ† ‰ΩøÁî®ÊñπÂºè
1. API Key Ë®≠ÂÆö
Ë´ãÂú®Âü∑Ë°åÁ®ãÂºèÊôÇËº∏ÂÖ• API KeyÔºåÊàñÊòØÈ†êÂÖàÂú®Áí∞Â¢ÉËÆäÊï∏‰∏≠Ë®≠ÂÆöÔºö
`export OPENAI_API_KEY="your_key_here"`

2. Âª∫Á´ã Scraper ‰∏¶Âü∑Ë°å
```python
import asyncio
from scraper import WikipediaScraper  # ÂÅáË®≠‰Ω†Êää class Â≠òÂú® scraper.py

urls = [
    "https://en.wikipedia.org/wiki/Natural_language_processing",
    "https://en.wikipedia.org/wiki/Machine_learning"
]

scraper = WikipediaScraper(base_urls=urls)

results = asyncio.run(scraper.scrape_multiple())

for r in results:
    print(r["title"], r["markdown_length"], "chars")
```

#### üìÇ ÂõûÂÇ≥ÁµêÊûúÊ†ºÂºè

`scrape_article` ÂõûÂÇ≥ÁöÑÁµêÊûúÊúÉÊòØ JSON-like dictÔºö
```bash
{
  "title": "Natural language processing",
  "html": "...",
  "markdown": "...",
  "html_length": 15234,
  "markdown_length": 8921,
  "links_found": 245,
  "crawl_time": 1735712456.2381
}
```

#### üßπ Ê∏ÖÁêÜÂÖßÂÆπ
‰Ω†ÂèØ‰ª•‰ΩøÁî® clean_content() Â∞á Markdown ËΩâÊèõÊàê‰πæÊ∑®ÁöÑÁ¥îÊñáÂ≠óÔºö
```python
raw_markdown = results[0]["markdown"]
clean_text = scraper.clean_content(raw_markdown)

print(clean_text[:500])  # È°ØÁ§∫Ââç 500 Â≠ó
```

`clean_content` ÂõûÂÇ≥ÁöÑÁµêÊûúÊúÉÊòØ JSON-like stringÔºö
```bash
{
    "title": "Natural language processing",
    "content" {
        "History": "...",
        "See also": "...",
    }
}
```
#### üîí OpenAI Client Âª∫Á´ãËàáÊ∏¨Ë©¶

Ê≠§Â∞àÊ°àÂÖßÂª∫ `create_secure_openai_client()`ÔºåÊúÉÔºö
- ÂæûÁí∞Â¢ÉËÆäÊï∏ËÆÄÂèñ API key
- Ê∏¨Ë©¶ÊòØÂê¶ËÉΩÊ≠£Á¢∫ÈÄ£Á∑ö
- ÂõûÂÇ≥‰∏ÄÂÄã OpenAI client

ÁØÑ‰æãÔºö
```python
from scraper import create_secure_openai_client

client = create_secure_openai_client()

if client:
    response = client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[{"role": "user", "content": "Hello, world!"}]
    )
    print(response.choices[0].message)
```

---

## üì¶ Data ContractsÔºàPydantic SchemasÔºâ

### Article Schema
```python
from pydantic import BaseModel, Field
from typing import List

class WikipediaExtraction(BaseModel):
    title: str = Field(description="Article's Title")
    description: str = Field(description="Article's Summary or Description or Overview")
    advantages: List[str] = Field(description="The advantages of the topic mentioned in the article")
    disadvantages: List[str] = Field(description="Known challenges or limitations")
    related_concepts: List[str] = Field(description="Related technology, see also")
    notable_methods: List[str] = Field(description="Notable methods, models, or techniques in this area")
```
> Ë®ªÔºöÂØ¶ÈöõÊ¨Ñ‰ΩçÂèØ‰æù‰ΩúÊ•≠ÊúÄÁµÇ schemaÔºàÂ¶Ç `summary`, `key_concepts`, `applications`ÔºâË™øÊï¥„ÄÇ

---

## üß† Agent ClassÔºàÁãÄÊÖãËàáË°åÁÇ∫Ôºâ

```python
class Agent:
    def __init__(self, model: str = "gpt-4o-mini"):
        self.articles: List[WikipediaExtraction] = []
        self.model = model
        self.create_secure_openai_client()

    def create_secure_openai_client(self):
    """
    Create OpenAI client with secure API key handling.

    This function:
    1. Looks for OPENAI_API_KEY in environment variables
    2. Tests the connection with a simple API call
    3. Returns the client or None if setup fails
    """
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("‚ùå No OPENAI_API_KEY found in environment variables")
        print("üí° Set environment variable: OPENAI_API_KEY=your_key")
        return None
    try:
        self.client = OpenAI(api_key=api_key)
        # Test connection with a simple API call
        models = client.models.list()
        print("‚úÖ OpenAI client created and tested successfully")

    except Exception as e:
        print(f"‚ùå OpenAI client creation failed: {e}")
        print("üîç Check your API key and internet connection")
        self.client = None

    # ---- Data In/Out ----
    def add_article(self, article: WikipediaExtraction) -> None:
        self.articles.append(article)

    # def list_titles(self) -> List[str]:
    #     return [a.title for a in self.articles]

    # def get_article_by_title(self, title: str) -> WikipediaExtraction:
    #     for a in self.articles:
    #         if a.title == title:
    #             return a
    #     raise ValueError(f"Article not found: {title}")

    # def get_articles_by_category(self, category: str) -> List[WikipediaExtraction]:
    #     # category ÂèØËÉΩÂ∞çÊáâ related_conceptsÔºåÈúÄËá™ÂÆöÁæ©
    #     results = [a for a in self.articles if category in a.related_concepts]
    #     if not results:
    #         raise ValueError(f"No articles found for category: {category}")
    #     return results

    # ---- Extraction ----
    def extract_structured_data(self, content: str, model: str = "gpt-4o-mini") -> WikipediaExtraction:
        """Use OpenAI structured output to extract data"""
        """
        Extract structured data from raw text using OpenAI's Structured Outputs.

        This replaces the old pattern of:
        response.json()  # Hope it works!

        With guaranteed schema compliance.
        """

        if not client:
            print("‚ö†Ô∏è Demo mode: Would extract structured data with OpenAI API")
            return create_mock_wiki_extraction()
        
        # Create the schema for OpenAI
        schema = {
            "name": "wiki_extraction",
            "schema": WikipediaExtraction.model_json_schema(),
            "strict": False  # This enforces strict schema compliance
        }

        try:
            # The magic happens here - response_format enforces our schema
            response = client.chat.completions.create(
                model=self.model,  # Only gpt-4o and gpt-4o-mini support structured outputs
                messages=[
                    {
                        "role": "system",
                        "content": "Please carefully read and comprehend the entire article content provided below. Extract all relevant information and structure it exactly according to the provided schema."
                    },
                    {
                        "role": "user",
                        "content": f"Please carefully analyze the article {content} below. Even if some fields within the schema are not explicitly provided by the article, use inference to fill in any missing information where reasonable. Then, extract and organize the data from the article according to the schema, ensuring completeness and accuracy based on both explicit statements and logical inference."
                    }
                ],
                response_format={
                    "type": "json_schema",
                    "json_schema": schema
                }
            )

            # Direct validation - no parsing errors possible!
            wiki_extraction = WikipediaExtraction.model_validate_json(response.choices[0].message.content)

            print("‚úÖ Structured extraction successful!")
            print(f"üìö Processed article: {wiki_extraction.title}")
            print(f"üí∞ Topic's Description: {wiki_extraction.description}")
            print(f"‚≠ê Topic's Advantages: {wiki_extraction.advantages}")
            print(f"üè∑Ô∏è Topic's Disadvantages: {wiki_extraction.disadvantages}")

            return wiki_extraction

        except Exception as e:
            print(f"‚ùå Extraction failed: {e}")
            return self.create_mock_wiki_extraction(content)
        
    def batch_extract(self, articles: List[Dict]) -> List[WikipediaExtraction]:
        """Process multiple articles"""
        extracted = []
        for article in articles:
            extracted.append(self.extract_structured_data(article['markdown']))
        
        return extracted
        

    def create_mock_wiki_extraction(self, raw_content: str):
        """Create mock data for demonstration when API is not available."""
        return WikipediaExtraction(
            title="Null",
            description="Null",
            advantages=[],
            disadvantages=[],
            related_concepts=[],
            notable_methods=[]
        )
```

### Function Calling Layer (Â∞çÂ§ñ‰ªãÈù¢)
```python
def ask_ai(self, query: str, assistant: Agent):
    """
    Function Calling Â±§ÔºåAI Assistant ËàáÂ§ñÈÉ®Â∞çË©±ÁöÑÂîØ‰∏ÄÂÖ•Âè£„ÄÇ
    - query: ‰ΩøÁî®ËÄÖÁöÑ‰ªªÂãô (ex: "compare_technologies: LLM, RNN")
    - assistant: Â∑≤Á∂ìÊìÅÊúâ structured articles ÁöÑ Agent
    """
    if query.startswith("compare_technologies"):
        # Example: compare_technologies: LLM, RNN
        techs = query.split(":")[1].split(",")
        results = [assistant.get_article_by_title(t.strip()) for t in techs]
        return results

    elif query.startswith("trace_evolution"):
        # Example: trace_evolution: Deep Learning
        topic = query.split(":")[1].strip()
        return assistant.get_articles_by_category(topic)

    else:
        raise ValueError(f"Unknown query: {query}")
```

**Error Handling**Ôºö  
- Ëã• `articles` ÁÇ∫Á©∫ÊàñÊâæ‰∏çÂà∞Ê®ôÈ°å ‚Üí `ValueError`„ÄÇ  
- `category` ÂèØÁÇ∫ `None`Ôºõ‰ΩøÁî®ÊôÇÈúÄÂÅöÁ©∫ÂÄºÊ™¢Êü•„ÄÇ  
- Ëº∏ÂÖ• JSON ‚Üí ‰ª• `Article.model_validate()` È©óË≠â„ÄÇ  

---

## üîß Function Calling ToolsÔºàÂ§ñÈÉ®Â∑•ÂÖ∑Ë¶èÊ†ºÔºâ

> Function Calling Á≤æÁ•ûÔºö**LLM Ê±∫Á≠ñ**ÊòØÂê¶ÂëºÂè´Â∑•ÂÖ∑Ôºå‰ΩÜÂ∑•ÂÖ∑Êú¨Ë∫´ÊòØ**Âõ∫ÂÆö„ÄÅÂèØÊéß**ÁöÑÈÇèËºØÔºõËã•ÁÑ° API ‚Üí ÈÄ≤ÂÖ• **Mock Mode**„ÄÇ  
> ÈÄôÂÖ©ÂÄãÂáΩÂºè‰ΩúÁÇ∫„ÄåÂ§ñÈÉ®Â∑•ÂÖ∑„ÄçÔºå‰∏çÂ∞ÅË£ùÂú® Class ÂÖßÔºàÁî± Class Êèê‰æõË≥áÊñôÁµ¶ÂÆÉÂÄëÔºâ„ÄÇ

### 1) `compare_technologies`
**Signature**
```python
def compare_technologies(article_a_json: str, article_b_json: str) -> str:
    """
    ÊØîËºÉÂÖ©ÂÄãÊäÄË°ìÊñáÁ´†Ôºà‰ª• JSON Â≠ó‰∏≤Ëº∏ÂÖ•Ôºâ‰∏¶ÂõûÂÇ≥ JSON Â≠ó‰∏≤ÁµêÊûú„ÄÇ

    Parameters:
        article_a_json (str): JSON string of Article
        article_b_json (str): JSON string of Article

    Returns:
        str: JSON string of ArticleComparison
    """
```
**Ëº∏ÂÖ•/Ëº∏Âá∫Ê†ºÂºè**  
- **Ëº∏ÂÖ•**Ôºö`Article` Áâ©‰ª∂Â∫èÂàóÂåñÂæåÁöÑ **JSON Â≠ó‰∏≤**Ôºà‰æøÊñº LLM ÊàñË∑®Ê®°ÁµÑÂÇ≥ÈÅûÔºâ„ÄÇ  
- **Ëº∏Âá∫**Ôºö`ArticleComparison` Áâ©‰ª∂Â∫èÂàóÂåñÂæåÁöÑ **JSON Â≠ó‰∏≤**„ÄÇ  

**Ë°åÁÇ∫ÔºàAPI / MockÔºâ**  
- **API ÂèØÁî®**ÔºöÂèØÂëºÂè´ Gemini/OpenAI Â∞çÂÖ©ÁØáÊñáÁ´†ÂÅöÂ∞çÊØîÊëòË¶ÅËàáÊ¨Ñ‰ΩçÂ°´ÂÖÖ„ÄÇ  
- **Mock**ÔºöÊì∑Âèñ `title`„ÄÅ`content` Ââç N Â≠óË£Ω‰ΩúÊëòË¶ÅÔºõ`common_concepts/unique_*` ÂèØÂü∫ÊñºÁ∞°ÂñÆÈóúÈçµÂ≠óÈõÜÂêàÊØîËºÉ„ÄÇ  

### 2) `trace_evolution`
**Signature**
```python
from typing import List
def trace_evolution(topic: str, articles_json: List[str]) -> str:
    """
    ËøΩËπ§ÂñÆ‰∏Ä‰∏ªÈ°åÂú®Â§öÁØáÊñáÁ´†‰∏≠ÁöÑÊºîÈÄ≤Ôºà‰ª• JSON Â≠ó‰∏≤Èô£ÂàóËº∏ÂÖ•„ÄÅJSON Â≠ó‰∏≤Ëº∏Âá∫Ôºâ„ÄÇ

    Parameters:
        topic (str): ÊäÄË°ì‰∏ªÈ°å
        articles_json (List[str]): list of JSON string of Article

    Returns:
        str: JSON string (e.g., {'timeline': [...], 'key_innovations': [...], 'notes': str})
    """
```
**Ë°åÁÇ∫ÔºàAPI / MockÔºâ**  
- **API ÂèØÁî®**ÔºöË´ã LLM ‰æùÊôÇÈñì/ÊÆµËêΩÊé®Ê∏¨ÊºîÈÄ≤ËÑàÁµ°ËàáÈáåÁ®ãÁ¢ë„ÄÇ  
- **Mock**Ôºö‰ª•Ê®ôÈ°åÊéíÂ∫è + ÂèñÊØèÁØáÁ¨¨‰∏ÄÊÆµ‰ΩúÁÇ∫„ÄåÈáåÁ®ãÁ¢ëÊëòË¶Å„Äç„ÄÇ  

---

## üîÑ Pipeline Âª∫Ë≠∞ÔºàÈÅøÂÖç globalÔºåÊ∏ÖÊ•öË≥áÊñôÊµÅÔºâ

1. **Áà¨Ëü≤** ‚Üí ÂèñÂæó raw HTML / text„ÄÇ  
2. **Ê∏ÖÁêÜ** ‚Üí `clean_content`ÔºàÁßªÈô§ÈõúË®ä„ÄÅ‰øùÁïôÊÆµËêΩÔºâ„ÄÇ  
3. **HTML Parser** ‚Üí Êì∑ÂèñÊ®ôÈ°å/ÊÆµËêΩ ‚Üí dict„ÄÇ  
4. **Pydantic** ‚Üí È©óË≠â ‚Üí `Article` Áâ©‰ª∂ ‚Üí Â≠òÂÖ• `Agent.articles`„ÄÇ  
5. **Function Calling**Ôºö
   - Âæû Class ÂèñÂá∫ `Article`ÔºåËΩâÊàê **JSON Â≠ó‰∏≤**Ôºå‰ΩúÁÇ∫Â∑•ÂÖ∑ÁöÑ **input arguments**„ÄÇ  
   - ÂëºÂè´ `compare_technologies` / `trace_evolution`ÔºàAPI Êàñ MockÔºâ„ÄÇ  
6. **ÁµêÊûúÂëàÁèæ**ÔºöÂç∞Âá∫ / Â≠òÊ™î / ÂèØË¶ñÂåñ„ÄÇ  

---

## ‚ö†Ô∏è Error Handling Á≠ñÁï•

- **Ê≤íÊúâ‰ªª‰Ωï Article**ÔºöÂú®ÂëºÂè´ÊØîËºÉ/ÊºîÈÄ≤ÂâçÂÖàÊ™¢Êü•ÔºåÂê¶Ââá `ValueError("No articles available")`„ÄÇ  
- **Êâæ‰∏çÂà∞ÊåáÂÆöÊ®ôÈ°å**Ôºö`get_article_by_title` ÊããÂá∫ `ValueError`„ÄÇ  
- **Category ÁÇ∫Á©∫**ÔºöÂÖÅË®± `None`ÔºõÂú®Ê™¢Á¥¢ÁâπÂÆöÂàÜÈ°ûÊôÇÈ°ØÁ§∫Ë≠¶ÂëäÊàñË∑≥ÈÅé„ÄÇ  
- **JSON Â∫èÂàóÂåñ/ÂèçÂ∫èÂàóÂåñ**Ôºö‰ΩøÁî® `Article.model_validate_json()` Ëàá `model_dump_json()`„ÄÇ  

---

## üîó JSON Ê†ºÂºèÁØÑ‰æãÔºàArticleÔºâ

```json
{
  "title": "Radiosity (Computer Graphics)",
  "content": "Radiosity is a global illumination algorithm ...",
  "category": "Rendering"
}
```

---

## üìé ‰ΩøÁî®Âª∫Ë≠∞
- ÂÖßÈÉ®ÈÅãÁÆóÂª∫Ë≠∞‰ª• **dict/Pydantic** ÈÄ≤Ë°åÔºõËàá LLM ÊàñÂ∑•ÂÖ∑Â±§‰∫§‰∫íÊôÇÂÜçËΩâ **JSON string**„ÄÇ  
- ‰øùÊåÅÂ∑•ÂÖ∑ÔºàÂáΩÂºèÔºâËº∏ÂÖ•/Ëº∏Âá∫Âö¥Ê†ºÂÆöÁæ©ÔºåÊñπ‰æøÊ∏¨Ë©¶ËàáÊõøÊèõÔºàAPI ‚Üî MockÔºâ„ÄÇ  
- ÈÅøÂÖç‰ΩøÁî® global stateÔºõÁî± Class Áµ±‰∏ÄÁÆ°ÁêÜÁãÄÊÖãËàáË≥áÊñô„ÄÇ

üîÆ Êú™‰æÜÊì¥ÂÖÖÂª∫Ë≠∞
- Ë≥áÊñôÂÑ≤Â≠òÔºöÂ∞áÁµêÊûúÂ≠òÂÖ• SQLite / MongoDB / CSV
- Ëá™Âãï ChunkingÔºö‰ΩøÁî® RegexChunking / SlidingWindowChunking
- Ë≥áË®äÊì∑ÂèñÔºöÊê≠ÈÖç LLMExtractionStrategy Ëá™ÂãïÁîüÊàêÁµêÊßãÂåñÁü•Ë≠ò
- ÂâçÁ´ØÂ±ïÁ§∫ÔºöÊï¥ÂêàÊàê‰∏ÄÂÄãÂ∞èÂûã Web NotebookLM Demo
